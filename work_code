import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

date_filter = lambda Dat: ((Dat["year"].to_numpy() > 1964) & (Dat["year"].to_numpy() < 2023)) | ((Dat["year"].to_numpy() == 1964) & (Dat["month"].to_numpy() > 1)) | ((Dat["year"].to_numpy() == 2023) & (Dat["month"].to_numpy() < 4)) 

def date_to_yearmonth(Dat):
    # Add columns year and month, provided a column Date with the year and month in the format 199901 for Jan 1999
    Dat["year"] = np.nan
    Dat["month"] = np.nan
    for i in Dat.index:
        try:
            date = int(Dat.loc[i, "Date"])
        except:
            Dat = Dat.loc[0:i -1, :]
            break
        year = int(date/100)
        month = date - year * 100
        Dat.loc[i, "year"] = year
        Dat.loc[i, "month"] = month

    return Dat

def import_data():
    Dat = date_to_yearmonth(pd.read_csv("Data.csv"))
    filter = date_filter(Dat=Dat)
    return Dat.loc[filter, :]

def import_Rf():
    Dat = date_to_yearmonth(pd.read_csv("Rfs.csv"))
    filter = date_filter(Dat=Dat)
    return Dat.loc[filter, :]


Data = import_data().set_index("Date")
Rfs = import_Rf().set_index("Date")

Data.loc[:, 'RF'] = Rfs.loc[:, "RF"]

# Different categories, i.e. the 25 portfolios
Categories = []
for column in Data.columns:
    if column == "Date" or column == "year" or column == "month" or column == "RF":
        continue
    Categories.append(column)

# calculating excess return for each category per month
for column in Categories:
    Data.loc[:, column + '_excess'] = Data.loc[:, column] - Data.loc[:, "RF"]

# calculating expected returns and covariance matrix
Results1 = pd.DataFrame(columns = ['E_Ri', 'Var_Ri'], index = Categories) # im storing the expected excess returns and variances of each portfolio here just to be able to print and visualize. For calculations I will use the numpy arrays below
Cov_Matrix = []
mu = []
for category in Categories:
    Results1.loc[category, "E_Ri"] = np.mean(Data.loc[:, category + '_excess'].to_numpy())
    Results1.loc[category, "Var_Ri"] = np.var(Data.loc[:, category + '_excess'].to_numpy(), ddof=1) # ddof = 1 makes the numerator of the estimator be N-1 instead of N
    mu.append(np.mean(Data.loc[:, category + '_excess'].to_numpy()))
    Cov_Matrix.append(Data.loc[:, category + '_excess'].to_numpy())

mu = np.vstack(mu) # making array vertical
Cov_Matrix = np.cov(np.array(Cov_Matrix))
Cov_Matrix_Inv = np.linalg.inv(Cov_Matrix) # expensive calculation but who cares

rf = 0.15
mu = mu + rf # 0.15 is risk free rate to obtain gross return

ones_vec = np.ones((25,1)) # vertical ones vector

A = np.dot(np.transpose(mu), np.dot(Cov_Matrix_Inv, mu))
B = np.dot(np.transpose(mu), np.dot(Cov_Matrix_Inv, ones_vec))
C = np.dot(np.transpose(ones_vec), np.dot(Cov_Matrix_Inv, ones_vec))


# GMV and mu PPortfolio (writing PPortfolio wrong so it is distinguishable from the initial 25 portfolios) -> PPortfolio = a portfolio of portfolios
pi_GMV = 1/C * np.dot(Cov_Matrix_Inv, ones_vec) # weights vector for GMV
mu_GMV = B/C
var_GMV = 1/C

pi_mu = 1/B  * np.dot(Cov_Matrix_Inv, mu) # weights vector for GMV
mu_mu = np.dot(np.transpose(mu), pi_mu)


weight_lambda = lambda mu_target: (B*C*mu_target - B*B)/(A*C - B*B) # function to get the weight of PPortfolios GMV and mu, from a target return

pi_target = lambda mu_target: weight_lambda(mu_target=mu_target) * pi_mu + (1-weight_lambda(mu_target=mu_target)) * pi_GMV  # function to get the weight of each portfolio based on the target return

mu_target_res = lambda mu_target: np.dot(np.transpose(mu), pi_target(mu_target=mu_target))

var_target = lambda mu_target: (A-2*B*mu_target+C*mu_target*mu_target)/(A*C - B*B) # function to get the variance of PPortfolios

# a vector of target returns
targets_mu = np.arange(0.1, 3.05, 0.01) * mu_GMV
# calculating the respective variances on the efficient frontier
targets_var = []
for mu_target in targets_mu:
    targets_var.append(var_target(mu_target=mu_target)[0])
targets_var = targets_var[0]
targets_mu = targets_mu[0]

# plotting
plt.plot(var_GMV, mu_GMV, marker="o")
plt.plot(var_target(mu_target=mu_mu), mu_mu, marker="o")
plt.plot(targets_var, targets_mu)

for category in Categories:
    plt.plot(Results1.loc[category, "Var_Ri"], Results1.loc[category, "E_Ri"] + rf, marker=".", color='k')

plt.ylabel('gross return')
plt.xlabel('volatility (variance)')
plt.show()

def grs_z(r, Rf = 0.15):
    r = np.array(r)
    rm = sum(r)/len(r) #avg or expected returns on market
    T = len(r) #lenght of data
    print(T)
    n = 1 #number of variables
    sh_rm_r = np.mean(r - rm) / np.std(r) #calculates Sharpe(rm, r) ration under the assumption that r is a array
    sh_r = np.mean(r - Rf)/ np.std(r)
    z = ((T - n - 1)/n) * (sh_rm_r **2 - sh_r **2)/ (1 + sh_r **2)
    print(z)
    critical_value = f.ppf(1 - 0.05, n, T - n) #F- distribution for 0.05 sign. lvl.
    if z > critical_value:
        return 'Reject null hypothesis, CAPM model is ok'
    else:
        return 'Accapt null hypothesis, CAPM is shit'

